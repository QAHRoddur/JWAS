{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# misc\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QAHRoddur/JWAS/blob/main/FAQ/misc.ipynb)\n",
        "\n",
        "This notebook is auto-generated from the JWAS.jl wiki page.\n"
      ],
      "id": "3541746514e6",
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Pkg\n",
        "Pkg.add(\"JWAS\")\n",
        "Pkg.precompile()\n",
        "using JWAS\n"
      ],
      "id": "a866c457a796",
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following functions will use `openblas` in julia to enable parallel computing:\n",
        "\n",
        "1. matrix * matrix  (`*`)\n",
        "\n",
        "2. matrix * vector  (`*`)\n",
        "\n",
        "3. dot (`dot()`)\n",
        "\n",
        "* The maximum number of cores `openblas` allows is #.\n",
        "* distributed for loop\n",
        "* dot() is speed up by `BLAS. dot()`, which is a function in BLAS.\n",
        "\n",
        "**Progress:**\n",
        "\n",
        "1. change `.` to `@.`, speed almost same. \n",
        "\n",
        "2. test:\n",
        "\n",
        "   X'y   \n",
        "\n",
        "   BLAS.gemv('T',X,y) #<-faster\n",
        "\n",
        "2. test:\n",
        "\n",
        "   Xa*[mu;α]:                            when Xa is UpperTriangular Array.\n",
        "\n",
        "   BLAS.trmv('U', 'N', 'N', Xa, [mu;α]): where Xa is normal Array.        \n",
        "   \n",
        "   Result: same speed (because Julia use BLAS.trmv for UpperTriangular matrix)\n",
        "\n",
        "3. test:\n",
        "   \n",
        "   Xa'ya:                            when Xa is UpperTriangular Array.\n",
        "   \n",
        "   BLAS.trmv('U', 'T', 'N', Xa, ya): where Xa is normal Array\n",
        "   \n",
        "   Result: same speed (because Julia use BLAS.trmv for UpperTriangular matrix)\n",
        "\n",
        "4. speed up cholesky decomposition by BLAS:\n",
        "       \n",
        "    LAPACK.potrf!('U', BB)\n",
        " \n",
        "    Xa = UpperTriangular(BB)\n",
        "\n",
        "5. speed up deriving max eigenvalue by BLAS:\n",
        "   \n",
        "   tmp = muX'muX\n",
        "\n",
        "   LAPACK.syev!('N', 'U', tmp)[end]\n",
        "\n",
        "6. test BLAS on windows(IIBLMM_BLAS.jl).\n",
        "   \n",
        "   `BLAS.vendor()` -> openblas64\n",
        "   \n",
        "   set_num_threads(1) : 60s\n",
        "\n",
        "   set_num_threads(2) : 57s\n",
        "\n",
        "   set_num_threads(3) : 57s\n",
        "\n",
        "   set_num_threads(4) : 62s\n",
        "\n",
        "   set_num_threads(8) : 57s\n",
        "\n",
        "7. test BLAS on server(farm).\n",
        "\n",
        "   `BLAS.vendor()` -> \n",
        "\n",
        "8. test BLAS on server(Gausi).\n",
        "\n",
        "**Next step:**\n",
        "\n",
        "1. BLAS is multi-thread function, how to speed up BLAS by setting more threads? \n",
        "\n",
        "    In windows, setting different BLAS threads seems have no difference. This may be the problem of my laptop. I need to test in server.(try both gausi and farm server)\n",
        "\n",
        "**Ideas on paper:**\n",
        "\n",
        "1. Xa is UpperTriangular matrix. So we can use BLAS function to achieve faster speed than normal matrix. In fact, Julia will use BLAS if input matrix is UpperTriangular.\n",
        "\n",
        "    If type of Xa is normal matrix, time:     93s  \n",
        "\n",
        "    If type of Xa is UpperTriangular, time:   58s  \n",
        "\n",
        "    If type of Xa is normal matrix+BLAS, time:57s  \n",
        "\n",
        "2. Correct typo in paper.\n",
        "\n",
        "3. can also use BLAS function to get max eigenvalue and do cholesky decomposition. (In paper, you only mentioned GPU to speed up.)\n",
        "\n",
        "**BLAS:**\n",
        "\n",
        "1.8  Q Is number of thread limited?\n",
        "\n",
        "       Basically, there is no limitation about number of threads. You\n",
        "       can specify number of threads as many as you want, but larger\n",
        "       number of threads will consume extra resource. I recommend you to\n",
        "       specify minimum number of threads.\n"
      ],
      "id": "2958f860ce3c",
      "metadata": {}
    }
  ],
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "misc.ipynb"
    },
    "language_info": {
      "file_extension": ".jl",
      "name": "julia"
    },
    "kernelspec": {
      "name": "julia-1.11",
      "display_name": "Julia 1.11",
      "language": "julia"
    }
  },
  "nbformat": 4
}